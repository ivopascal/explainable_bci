# explainable_bci
A repo where we apply XAI methods to see where an EEG classifier is looking. This can indicate whether the understanding of the CNN is cognitively plausible.

This is applied to Error Related Potentials classification, where the explanations are generated with SHAP.
`uncorrected_dataset.pkl` is available upon request. 

We find that indeed the explanations point to the phenomena that should be detected.

![Figure_1](https://github.com/user-attachments/assets/2ec12154-fb8e-47be-824d-24a420527284)
